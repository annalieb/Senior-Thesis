data.Senic <- read.table(file.choose(),header=TRUE)
attach(data.Senic)
head(data.Senic)
# 1a
plot(Stay, InfctRsk)
# 1b
senic.lm <- lm(InfctRsk~Stay, data=data.Senic)
summary(senic.lm)
abline(senic.lm)
# 1d
newdata <- data.Senic[Stay<=15,]
# 1c
# MSE = sigma^2 = (Residual standard error)^2
1.139^2
library(leaps)
install.packages(leaps)
install.packages("leaps")
library(leaps)
chem <- read.table(file.choose(),header=TRUE)
leaps(X = chem[, -1], y=y, method="Cp")
?leaps
leaps(x=chem[, -1], y=y, method="Cp")
leaps(x=chem[, -1], y=chem$y, method="Cp")
all_subset_Cp = leaps(x=chem[, -1], y=chem$y, method="Cp")
which.min(all_subset_Cp$Cp)
all_subset_Cp$which
all_subset_Cp$which[31]
chem_lm <- lm(y~., data=chem)
chem_lm
summary(chem_lm)
names(chem[, -c(1, 5)])
names(chem[, -c(-1, 5)])
names(chem[, -c(11, 5)])
all_subset_Cp = leaps(x=chem[, -c(11, 5)], y=chem$y, method="Cp")
which.min(all_subset_Cp$Cp)
all_subset_Cp$which[31]
all_subset_Cp$which[31,]
names(chem[, -c(11, 5)])
best_model <- chem[, -c(1, 3, 5, 6, 8, 9 11)]
best_model <- lm(y~., data=chem[, -c(1, 3, 5, 6, 8, 9 11)])
summary(best_model)
best_model <- lm(y~., data=chem[, -c(1, 3, 5, 6, 8, 9, 11)])
best_model <- lm(y~., data=chem[, -c(1, 3, 5, 6, 8, 9, 11)])
summary(best_model)
best_model <- lm(y~., data=chem[, -c(1, 3, 5, 6, 8, 9)])
summary(best_model)
all_subset_Cp = leaps(x=chem[, -c(5, 11)], y=chem$y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
ind
all_subset_Cp$which[ind,]
names(chem[, -c(5, 11)])
best_model <- lm(y~., data=chem[, -c(4, 5, 6, 7, 8, 9)])
summary(best_model)
ind
all_subset_Cp$which[ind,]
names(chem[, -c(5, 11)])
chem_lm <- lm(y~., data=chem)
summary(chem_lm)
# x5 is excluded: perfect linear collinearity with another var in the model
all_subset_Cp = leaps(x=chem[, -c(5, 11)], y=chem$y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
ind
names(chem[, -c(5, 11)])
all_subset_Cp$which[ind,]
best_model <- lm(y~x1+x2+x3+x10, data=chem)
summary(best_model)
library(usdm)
install.packages("usdm")
library(usdm)
# check regression assumptions
# multicollinearity
vifstep(x=chem[, c(1, 2, 3, 10)], th=10)
?pairs
# linearity assumption
pairs(chem[, c(1, 3, 10, 11)])
# remove x2 due to multicollinearity problem
best_model <- lm(y~x1+x3+x10, data=chem)
# remove x2 due to multicollinearity problem
best_model_2 <- lm(y~x1+x3+x10, data=chem)
# check residuals plot
plot(best_model_2$resid)
# check residuals plot
plot(best_model_2)
# check residuals plot and qqnorm
plot(best_model_2)
plot(best_model_2$resid)
# box-cox transformation
boxcox(best_model_2)
library(MASS)
# box-cox transformation
boxcox(best_model_2)
vifstep(x=chem[, -c(11)], th=10)
summary(chem_lm)
# check regression assumptions
vifstep(x=chem[, c(1, 2, 3, 10)], th=10)
# check residuals plot and qqnorm
plot(best_model_2)
# box-cox transformation
boxcox(best_model_2)
range(chem$y)
chem_transformed$y <- (chem_transformed$y)^(-1)
final_model <- lm(y~x1+x3+x10, data = chem_transformed)
chem_transformed <- chem
chem_transformed$y <- (chem_transformed$y)^(-1)
final_model <- lm(y~x1+x3+x10, data = chem_transformed)
plot(final_model)
mydata.new <- chem[, -c(5)]
mydata.new <- chem[, -c(5)]
# 1b
lm.additive <- lm(y~.,data=mydata.new)
# for AIC (k = 2)
step(lm.additive,.~.^2,direction="both",k=2)
lm(x5~., data=chem)
summary(lm(x5~., data=chem))
# for BIC (k = log(n))
step(lm.additive,.~.^2,direction="both",k=log())
dim(mydata.new)
n <- dim(mydata.new)[1]
step(lm.additive,.~.^2,direction="both",k=log(n))
View(mydata.new)
best_BIC <- lm(y ~ x1 + x2 + x3 + x4 + x6 + x10 + x4:x10,
data = mydata.new)
vifstep(best_BIC)
# Question 2
animals <- read.csv(file.choose(),header=TRUE)
names(animals)
animals$Body_Weight_kg <- animals$Body_Weight / 1000
names(animals)
View(final_model)
plot(animals$Brain_Weight, animals$Body_Weight_kg)
View(chem)
plot(animals$Body_Weight_kg, animals$Brain_Weight)
animals$Body_Weight_kg
sum(animals$Body_Weight_kg < 1000)
sum(animals$Body_Weight_kg > 1000)
light_animals <- animals[animals$Body_Weight_kg > 1000]
light_animals <- animals[animals$Body_Weight_kg > 1000, ]
plot(light_animals$Body_Weight_kg, animals$Brain_Weight)
plot(light_animals$Body_Weight_kg, light_animals$Brain_Weight)
light_animals <- animals[animals$Body_Weight_kg < 1000, ]
plot(light_animals$Body_Weight_kg, light_animals$Brain_Weight)
# light_animals <- animals[animals$Body_Weight_kg < 1000, ]
plot(animals$Body_Weight_kg, animals$Brain_Weight)
View(chem_lm)
View(chem_transformed)
View(best_model)
model1 <- lm(Brain_Weight~Body_Weight_kg, data=animals)
abline(model1)
summary(model1)
# 2c
scatter.smooth(residuals(model1)~predict(model1))
?scatter.smooth
# 2c
scatter.smooth(residuals(model1)~predict(model1))
# the residuals vs. fitted values plot indicates that the
# constant variance assumption is violated.
light_animals <- animals[animals$Body_Weight_kg < 1000, ]
model1 <- lm(Brain_Weight~Body_Weight_kg, data=light_animals)
# 2c
scatter.smooth(residuals(model1)~predict(model1))
# 2c
scatter.smooth(residuals(model1)~predict(model1))
model1 <- lm(Brain_Weight~Body_Weight_kg, data=animals)
abline(model1)
summary(model1)
# 2c
scatter.smooth(residuals(model1)~predict(model1))
# 2d
which.max(animals$Body_Weight_kg > 10000)
animals[19,]
predict(model1, newdata=animals[19,])
# 2e
boxcox(model1)
?log
# use lambda = 0, --> apply a log transformation
model2 <- lm(log(Brain_Weight)~Body_Weight_kg, data=animals)
summary(model2)
summary(model1)
summary(model2)
# 2f
model3 <- lm(log(Brain_Weight)~log(Body_Weight_kg), data=animals)
summary(model3)
plot(model3)
# 2h
# scatter plot for part f
plot(log(Body_Weight_kg), log(Brain_Weight))
# 2h
# scatter plot for part f
plot(log(animals$Body_Weight_kg), log(animals$Brain_Weight))
abline(model3)
plot(animals$Body_Weight_kg, log(animals$Brain_Weight))
abline(model2)
plot(log(animals$Body_Weight_kg), log(animals$Brain_Weight))
abline(model3)
animals$Species_Code
# 2h
plot(log(animals$Body_Weight_kg),
log(animals$Brain_Weight),
col=animals$Species_Code)
# 2h
plot(log(animals$Body_Weight_kg),
log(animals$Brain_Weight),
pch=animals$Species_Code)
# 2h
plot(log(animals$Body_Weight_kg),
log(animals$Brain_Weight),
pch=(animals$Species_Code + 20))
abline(model3)
# y ~ x2 + x4 + x6 + x7 + x10 + x4:x10 + x7:x10 + x4:x6
vifstep(x=chem[, c(2, 4, 6, 7, 10)], th=10)
# Question 1
# 1a
chem <- read.table(file.choose(),header=TRUE)
chem_lm <- lm(y~., data=chem)
summary(chem_lm)
# x5 is excluded: perfect linear collinearity with another var
summary(lm(x5~., data=chem))
mydata.new <- chem[, -c(5)]
all_subset_Cp = leaps(x=mydata.new[, -11], y=mydata.new$y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
ind
mydata.new <- chem[, -5]
all_subset_Cp = leaps(x=mydata.new[, -11], y=mydata.new$y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
ind
all_subset_Cp$which[ind,]
names(mydata.new[, -11])
all_subset_Cp = leaps(x=mydata.new[, -10], y=mydata.new$y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
ind
all_subset_Cp$which[ind,]
names(mydata.new[, -10])
best_model <- lm(y~x1+x2+x3+x10, data=mydata.new)
summary(best_model)
names(mydata.new)
attach(mydata.new)
all_subset_Cp = leaps(x=mydata.new[, -10], y=y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
ind
all_subset_Cp$which[ind,]
# check residual plot
plot(best_model)
boxcox(best_model_2)
boxcox(best_model)
# transform with lambda = -1
range(mydata.new$y) # y-var is strictly positive
boxcox(best_model)
# transform with lambda = -1
range(mydata.new$y) # y-var is strictly positive
anova(best_model)
mydata.new$y <- mydata.new$y^(-1)
final_model <- lm(y~x1+x2+x3+x10, data = chem_transformed)
final_model <- lm(y~x1+x2+x3+x10, data = mydata.new)
which.max(mydata.new$y)
which.max(chem$y)
# 2c
scatter.smooth(residuals(model1)~predict(model1))
# Question 2
animals <- read.csv(file.choose(),header=TRUE)
# 2a
animals$Body_Weight_kg <- animals$Body_Weight / 1000
# light_animals <- animals[animals$Body_Weight_kg < 1000, ]
plot(animals$Body_Weight_kg, animals$Brain_Weight)
plot(animals$Body_Weight_kg, animals$Brain_Weight,
xlim=(400, 500), ylim=(-500, 1500))
plot(animals$Body_Weight_kg, animals$Brain_Weight,
xlim=c(400, 500), ylim=c(-500, 1500))
# hard to assess the relationship without performing transformation
plot(animals$Body_Weight_kg, animals$Brain_Weight,
xlim=c(0, 500), ylim=c(0, 1500))
model1 <- lm(Brain_Weight~Body_Weight_kg, data=animals)
abline(model1)
summary(model1)
scatter.smooth(residuals(model1)~predict(model1),
xlim=c(400, 500), ylim=c(-500, 1500))
# 2c
scatter.smooth(residuals(model1)~predict(model1))
# light_animals <- animals[animals$Body_Weight_kg < 1000, ]
plot(animals$Body_Weight_kg, animals$Brain_Weight)
# hard to assess the relationship without performing transformation
plot(animals$Body_Weight_kg, animals$Brain_Weight,
xlim=c(0, 500), ylim=c(0, 1500))
# 1c
best_BIC <- lm(y ~ x1 + x2 + x3 + x4 + x6 + x10 + x4:x10,
data = mydata.new)
which.max(chem$y)
predict(best_BIC, newdata=mydata[17,],
interval="confidence", )
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", )
# R-squared increased (better model fit)
# Multiple R-squared:  0.7845,	Adjusted R-squared:  0.7768
# scatter plot
plot(log(animals$Body_Weight_kg), log(animals$Brain_Weight))
# 2h
plot(log(animals$Body_Weight_kg),
log(animals$Brain_Weight),
pch=(animals$Species_Code + 20))
model4 <- lm(log(Brain_Weight)~log(Body_Weight_kg) + as.factor(Species_Code),
data=animals)
summary(model4)
# R-squared increased (better model fit)
# Multiple R-squared:  0.7845,	Adjusted R-squared:  0.7768
# scatter plot
plot(log(animals$Body_Weight_kg), log(animals$Brain_Weight))
plot(log(animals$Body_Weight_kg),
log(animals$Brain_Weight),
pch=(animals$Species_Code + 20))
# residual plot is better
plot(model3)
# 2f
model3 <- lm(log(Brain_Weight)~log(Body_Weight_kg), data=animals)
# residual plot is better
plot(model3)
final_model <- lm(y~x1+x2+x3+x10, data = mydata.new)
summary(final_model)
mydata.new$y <- mydata.new$y^(-1)
final_model <- lm(y~x1+x2+x3+x10, data = mydata.new)
summary(final_model)
chem_lm <- lm(y~., data=chem)
summary(chem_lm)
# x5 is excluded: perfect linear collinearity with another var
summary(lm(x5~., data=chem))
mydata.new <- chem[, -5]
attach(mydata.new)
all_subset_Cp = leaps(x=mydata.new[, -10], y=y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
ind
all_subset_Cp$which[ind,]
names(mydata.new[, -10])
best_model <- lm(y~x1+x2+x3+x10, data=mydata.new)
summary(best_model)
# check residual plot
plot(best_model)
mydata.new$y <- mydata.new$y^(-1)
final_model <- lm(y~x1+x2+x3+x10, data = mydata.new)
summary(final_model)
model1 <- lm(Brain_Weight~Body_Weight_kg, data=animals)
abline(model1)
summary(model1)
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", )
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", level = 0.9)
which.max(chem$y)
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", level = 0.9)
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", level = 0.90)
summary(best_BIC)
# Question 1
# 1a
chem <- read.table(file.choose(),header=TRUE)
full_lm <- lm(y~., data=chem)
summary(full_lm)
coefs(full_lm)
full_lm.coefs
full_lm.coefs()
?coef
coef(full_lm)
full_lm <- lm(y~., data=chem)
coef(full_lm)
coef(full_lm)
coef(full_lm)
# x5 is excluded: perfect linear collinearity with another var
summary(lm(x5~., data=chem))
# x5 is excluded: perfect linear collinearity with another var
lm(x5~., data=chem)$r.squared
# x5 is excluded: perfect linear collinearity with another var
summary(lm(x5~., data=chem))$r.squared
mydata.new <- chem[, -5]
all_subset_Cp = leaps(x=mydata.new[, -10], y=y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
all_subset_Cp$which[ind,]
names(mydata.new[, -10])
best_Cp <- lm(y~x1+x2+x3+x10, data=mydata.new)
summary(best_model)
summary(best_Cp)
# check residual plot
plot(best_model)
# check residual plot
plot(best_Cp)
# box-cox transformation
boxcox(best_model)
# box-cox transformation
boxcox(best_Cp)
final_model <- lm(y^(-1)~x1+x2+x3+x10, data = mydata.new)
summary(final_model) # intercept -9.195
final_model <- lm(y^(-1)~x1+x2+x3+x10, data = mydata.new)
plot(final_model)
plot(final_model)
summary(final_model)
# 1b
lm.additive <- lm(y~.,data=mydata.new)
# for AIC (k = 2)
step(lm.additive,.~.^2,direction="both",k=2)
n <- dim(mydata.new)[1]
step(lm.additive,.~.^2,direction="both",k=log(n))
best_BIC <- lm(y ~ x1 + x2 + x3 + x4 + x6 + x10 + x4:x10,
data = mydata.new)
which.max(chem$y)
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", level = 0.90)
summary(best_BIC)
n <- dim(mydata.new)[1]
step(lm.additive,.~.^2,direction="both",k=log(n))
# for AIC (k = 2)
step(lm.additive,.~.^2,direction="both",k=2)
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", level = 0.90)
# Question 2
animals <- read.csv(file.choose(),header=TRUE)
# 2a
animals$Body_Weight_kg <- animals$Body_Weight / 1000
plot(animals$Body_Weight_kg, animals$Brain_Weight)
plot(animals$Body_Weight_kg, animals$Brain_Weight,
xlim=c(0, 500), ylim=c(0, 1500))
# 2b
model1 <- lm(Brain_Weight~Body_Weight_kg, data=animals)
plot(animals$Body_Weight_kg, animals$Brain_Weight)
abline(model1)
summary(model1)
# 2c
scatter.smooth(residuals(model1)~predict(model1))
# seems to be at least one outlying observation.
# the residuals vs. fitted values plot indicates that the
# constant variance assumption is violated.
# try to zoom in
scatter.smooth(residuals(model1)~predict(model1),
xlim=c(400, 500), ylim=c(-500, 1500))
# 2d
which.max(animals$Body_Weight_kg > 10000)
animals[19,] # blue whale
max_ind <- which.max(animals$Body_Weight_kg > 10000)
animals[max_ind,] # blue whale
predict(model1, newdata=animals[19,])
animals[max_ind,] # blue whale
# 2e
boxcox(model1)
summary(model2)
# use lambda = 0, --> apply a log transformation
model2 <- lm(log(Brain_Weight)~Body_Weight_kg, data=animals)
summary(model2)
3.406e+00
1.047e-04
# 2f
model3 <- lm(log(Brain_Weight)~log(Body_Weight_kg), data=animals)
summary(model3)
# R-squared decreased (worse model fit)
# Multiple R-squared:  0.133,	Adjusted R-squared:  0.1021
plot(animals$Body_Weight_kg, log(animals$Brain_Weight))
abline(model2)
model3 <- lm(log(Brain_Weight)~log(Body_Weight_kg), data=animals)
summary(model3)
plot(log(animals$Body_Weight_kg), log(animals$Brain_Weight))
abline(model3)
# residual plot is better
plot(model3)
> plot(model3)
plot(model3)
# residual plot is much better!
scatter.smooth(residuals(model3)~predict(model3))
plot(log(animals$Body_Weight_kg),
log(animals$Brain_Weight),
pch=(animals$Species_Code + 20))
abline(model3)
# use Species_Code as a predictor
model4 <- lm(log(Brain_Weight)~log(Body_Weight_kg) + as.factor(Species_Code),
data=animals)
summary(model4)
setwd("/Users/annalieb/Documents/Thesis/")
labels <- read.csv("coverage_by_unique_headline.csv")
pwd
getwd()
setwd("/Users/annalieb/Documents/Thesis/Senior-Thesis")
labels <- read.csv("coverage_by_unique_headline.csv")
stances <- labels$stance
actor <- labels$actor
labels <- labels[, 6:7]
table(labels)
table(labels)
table(stances)
table(actor)
table(labels) / 11704
sum(table(labels) / 11704)
(table(labels) / 11704) * 100
table(actor) / 11704
table(stances) / 11704
(table(stances) / 11704) * 100
(table(actor) / 11704) * 100
