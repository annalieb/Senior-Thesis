summary(best_model)
ind
all_subset_Cp$which[ind,]
names(chem[, -c(5, 11)])
chem_lm <- lm(y~., data=chem)
summary(chem_lm)
# x5 is excluded: perfect linear collinearity with another var in the model
all_subset_Cp = leaps(x=chem[, -c(5, 11)], y=chem$y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
ind
names(chem[, -c(5, 11)])
all_subset_Cp$which[ind,]
best_model <- lm(y~x1+x2+x3+x10, data=chem)
summary(best_model)
library(usdm)
install.packages("usdm")
library(usdm)
# check regression assumptions
# multicollinearity
vifstep(x=chem[, c(1, 2, 3, 10)], th=10)
?pairs
# linearity assumption
pairs(chem[, c(1, 3, 10, 11)])
# remove x2 due to multicollinearity problem
best_model <- lm(y~x1+x3+x10, data=chem)
# remove x2 due to multicollinearity problem
best_model_2 <- lm(y~x1+x3+x10, data=chem)
# check residuals plot
plot(best_model_2$resid)
# check residuals plot
plot(best_model_2)
# check residuals plot and qqnorm
plot(best_model_2)
plot(best_model_2$resid)
# box-cox transformation
boxcox(best_model_2)
library(MASS)
# box-cox transformation
boxcox(best_model_2)
vifstep(x=chem[, -c(11)], th=10)
summary(chem_lm)
# check regression assumptions
vifstep(x=chem[, c(1, 2, 3, 10)], th=10)
# check residuals plot and qqnorm
plot(best_model_2)
# box-cox transformation
boxcox(best_model_2)
range(chem$y)
chem_transformed$y <- (chem_transformed$y)^(-1)
final_model <- lm(y~x1+x3+x10, data = chem_transformed)
chem_transformed <- chem
chem_transformed$y <- (chem_transformed$y)^(-1)
final_model <- lm(y~x1+x3+x10, data = chem_transformed)
plot(final_model)
mydata.new <- chem[, -c(5)]
mydata.new <- chem[, -c(5)]
# 1b
lm.additive <- lm(y~.,data=mydata.new)
# for AIC (k = 2)
step(lm.additive,.~.^2,direction="both",k=2)
lm(x5~., data=chem)
summary(lm(x5~., data=chem))
# for BIC (k = log(n))
step(lm.additive,.~.^2,direction="both",k=log())
dim(mydata.new)
n <- dim(mydata.new)[1]
step(lm.additive,.~.^2,direction="both",k=log(n))
View(mydata.new)
best_BIC <- lm(y ~ x1 + x2 + x3 + x4 + x6 + x10 + x4:x10,
data = mydata.new)
vifstep(best_BIC)
# Question 2
animals <- read.csv(file.choose(),header=TRUE)
names(animals)
animals$Body_Weight_kg <- animals$Body_Weight / 1000
names(animals)
View(final_model)
plot(animals$Brain_Weight, animals$Body_Weight_kg)
View(chem)
plot(animals$Body_Weight_kg, animals$Brain_Weight)
animals$Body_Weight_kg
sum(animals$Body_Weight_kg < 1000)
sum(animals$Body_Weight_kg > 1000)
light_animals <- animals[animals$Body_Weight_kg > 1000]
light_animals <- animals[animals$Body_Weight_kg > 1000, ]
plot(light_animals$Body_Weight_kg, animals$Brain_Weight)
plot(light_animals$Body_Weight_kg, light_animals$Brain_Weight)
light_animals <- animals[animals$Body_Weight_kg < 1000, ]
plot(light_animals$Body_Weight_kg, light_animals$Brain_Weight)
# light_animals <- animals[animals$Body_Weight_kg < 1000, ]
plot(animals$Body_Weight_kg, animals$Brain_Weight)
View(chem_lm)
View(chem_transformed)
View(best_model)
model1 <- lm(Brain_Weight~Body_Weight_kg, data=animals)
abline(model1)
summary(model1)
# 2c
scatter.smooth(residuals(model1)~predict(model1))
?scatter.smooth
# 2c
scatter.smooth(residuals(model1)~predict(model1))
# the residuals vs. fitted values plot indicates that the
# constant variance assumption is violated.
light_animals <- animals[animals$Body_Weight_kg < 1000, ]
model1 <- lm(Brain_Weight~Body_Weight_kg, data=light_animals)
# 2c
scatter.smooth(residuals(model1)~predict(model1))
# 2c
scatter.smooth(residuals(model1)~predict(model1))
model1 <- lm(Brain_Weight~Body_Weight_kg, data=animals)
abline(model1)
summary(model1)
# 2c
scatter.smooth(residuals(model1)~predict(model1))
# 2d
which.max(animals$Body_Weight_kg > 10000)
animals[19,]
predict(model1, newdata=animals[19,])
# 2e
boxcox(model1)
?log
# use lambda = 0, --> apply a log transformation
model2 <- lm(log(Brain_Weight)~Body_Weight_kg, data=animals)
summary(model2)
summary(model1)
summary(model2)
# 2f
model3 <- lm(log(Brain_Weight)~log(Body_Weight_kg), data=animals)
summary(model3)
plot(model3)
# 2h
# scatter plot for part f
plot(log(Body_Weight_kg), log(Brain_Weight))
# 2h
# scatter plot for part f
plot(log(animals$Body_Weight_kg), log(animals$Brain_Weight))
abline(model3)
plot(animals$Body_Weight_kg, log(animals$Brain_Weight))
abline(model2)
plot(log(animals$Body_Weight_kg), log(animals$Brain_Weight))
abline(model3)
animals$Species_Code
# 2h
plot(log(animals$Body_Weight_kg),
log(animals$Brain_Weight),
col=animals$Species_Code)
# 2h
plot(log(animals$Body_Weight_kg),
log(animals$Brain_Weight),
pch=animals$Species_Code)
# 2h
plot(log(animals$Body_Weight_kg),
log(animals$Brain_Weight),
pch=(animals$Species_Code + 20))
abline(model3)
# y ~ x2 + x4 + x6 + x7 + x10 + x4:x10 + x7:x10 + x4:x6
vifstep(x=chem[, c(2, 4, 6, 7, 10)], th=10)
# Question 1
# 1a
chem <- read.table(file.choose(),header=TRUE)
chem_lm <- lm(y~., data=chem)
summary(chem_lm)
# x5 is excluded: perfect linear collinearity with another var
summary(lm(x5~., data=chem))
mydata.new <- chem[, -c(5)]
all_subset_Cp = leaps(x=mydata.new[, -11], y=mydata.new$y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
ind
mydata.new <- chem[, -5]
all_subset_Cp = leaps(x=mydata.new[, -11], y=mydata.new$y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
ind
all_subset_Cp$which[ind,]
names(mydata.new[, -11])
all_subset_Cp = leaps(x=mydata.new[, -10], y=mydata.new$y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
ind
all_subset_Cp$which[ind,]
names(mydata.new[, -10])
best_model <- lm(y~x1+x2+x3+x10, data=mydata.new)
summary(best_model)
names(mydata.new)
attach(mydata.new)
all_subset_Cp = leaps(x=mydata.new[, -10], y=y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
ind
all_subset_Cp$which[ind,]
# check residual plot
plot(best_model)
boxcox(best_model_2)
boxcox(best_model)
# transform with lambda = -1
range(mydata.new$y) # y-var is strictly positive
boxcox(best_model)
# transform with lambda = -1
range(mydata.new$y) # y-var is strictly positive
anova(best_model)
mydata.new$y <- mydata.new$y^(-1)
final_model <- lm(y~x1+x2+x3+x10, data = chem_transformed)
final_model <- lm(y~x1+x2+x3+x10, data = mydata.new)
which.max(mydata.new$y)
which.max(chem$y)
# 2c
scatter.smooth(residuals(model1)~predict(model1))
# Question 2
animals <- read.csv(file.choose(),header=TRUE)
# 2a
animals$Body_Weight_kg <- animals$Body_Weight / 1000
# light_animals <- animals[animals$Body_Weight_kg < 1000, ]
plot(animals$Body_Weight_kg, animals$Brain_Weight)
plot(animals$Body_Weight_kg, animals$Brain_Weight,
xlim=(400, 500), ylim=(-500, 1500))
plot(animals$Body_Weight_kg, animals$Brain_Weight,
xlim=c(400, 500), ylim=c(-500, 1500))
# hard to assess the relationship without performing transformation
plot(animals$Body_Weight_kg, animals$Brain_Weight,
xlim=c(0, 500), ylim=c(0, 1500))
model1 <- lm(Brain_Weight~Body_Weight_kg, data=animals)
abline(model1)
summary(model1)
scatter.smooth(residuals(model1)~predict(model1),
xlim=c(400, 500), ylim=c(-500, 1500))
# 2c
scatter.smooth(residuals(model1)~predict(model1))
# light_animals <- animals[animals$Body_Weight_kg < 1000, ]
plot(animals$Body_Weight_kg, animals$Brain_Weight)
# hard to assess the relationship without performing transformation
plot(animals$Body_Weight_kg, animals$Brain_Weight,
xlim=c(0, 500), ylim=c(0, 1500))
# 1c
best_BIC <- lm(y ~ x1 + x2 + x3 + x4 + x6 + x10 + x4:x10,
data = mydata.new)
which.max(chem$y)
predict(best_BIC, newdata=mydata[17,],
interval="confidence", )
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", )
# R-squared increased (better model fit)
# Multiple R-squared:  0.7845,	Adjusted R-squared:  0.7768
# scatter plot
plot(log(animals$Body_Weight_kg), log(animals$Brain_Weight))
# 2h
plot(log(animals$Body_Weight_kg),
log(animals$Brain_Weight),
pch=(animals$Species_Code + 20))
model4 <- lm(log(Brain_Weight)~log(Body_Weight_kg) + as.factor(Species_Code),
data=animals)
summary(model4)
# R-squared increased (better model fit)
# Multiple R-squared:  0.7845,	Adjusted R-squared:  0.7768
# scatter plot
plot(log(animals$Body_Weight_kg), log(animals$Brain_Weight))
plot(log(animals$Body_Weight_kg),
log(animals$Brain_Weight),
pch=(animals$Species_Code + 20))
# residual plot is better
plot(model3)
# 2f
model3 <- lm(log(Brain_Weight)~log(Body_Weight_kg), data=animals)
# residual plot is better
plot(model3)
final_model <- lm(y~x1+x2+x3+x10, data = mydata.new)
summary(final_model)
mydata.new$y <- mydata.new$y^(-1)
final_model <- lm(y~x1+x2+x3+x10, data = mydata.new)
summary(final_model)
chem_lm <- lm(y~., data=chem)
summary(chem_lm)
# x5 is excluded: perfect linear collinearity with another var
summary(lm(x5~., data=chem))
mydata.new <- chem[, -5]
attach(mydata.new)
all_subset_Cp = leaps(x=mydata.new[, -10], y=y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
ind
all_subset_Cp$which[ind,]
names(mydata.new[, -10])
best_model <- lm(y~x1+x2+x3+x10, data=mydata.new)
summary(best_model)
# check residual plot
plot(best_model)
mydata.new$y <- mydata.new$y^(-1)
final_model <- lm(y~x1+x2+x3+x10, data = mydata.new)
summary(final_model)
model1 <- lm(Brain_Weight~Body_Weight_kg, data=animals)
abline(model1)
summary(model1)
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", )
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", level = 0.9)
which.max(chem$y)
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", level = 0.9)
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", level = 0.90)
summary(best_BIC)
# Question 1
# 1a
chem <- read.table(file.choose(),header=TRUE)
full_lm <- lm(y~., data=chem)
summary(full_lm)
coefs(full_lm)
full_lm.coefs
full_lm.coefs()
?coef
coef(full_lm)
full_lm <- lm(y~., data=chem)
coef(full_lm)
coef(full_lm)
coef(full_lm)
# x5 is excluded: perfect linear collinearity with another var
summary(lm(x5~., data=chem))
# x5 is excluded: perfect linear collinearity with another var
lm(x5~., data=chem)$r.squared
# x5 is excluded: perfect linear collinearity with another var
summary(lm(x5~., data=chem))$r.squared
mydata.new <- chem[, -5]
all_subset_Cp = leaps(x=mydata.new[, -10], y=y, method="Cp")
ind <- which.min(all_subset_Cp$Cp)
all_subset_Cp$which[ind,]
names(mydata.new[, -10])
best_Cp <- lm(y~x1+x2+x3+x10, data=mydata.new)
summary(best_model)
summary(best_Cp)
# check residual plot
plot(best_model)
# check residual plot
plot(best_Cp)
# box-cox transformation
boxcox(best_model)
# box-cox transformation
boxcox(best_Cp)
final_model <- lm(y^(-1)~x1+x2+x3+x10, data = mydata.new)
summary(final_model) # intercept -9.195
final_model <- lm(y^(-1)~x1+x2+x3+x10, data = mydata.new)
plot(final_model)
plot(final_model)
summary(final_model)
# 1b
lm.additive <- lm(y~.,data=mydata.new)
# for AIC (k = 2)
step(lm.additive,.~.^2,direction="both",k=2)
n <- dim(mydata.new)[1]
step(lm.additive,.~.^2,direction="both",k=log(n))
best_BIC <- lm(y ~ x1 + x2 + x3 + x4 + x6 + x10 + x4:x10,
data = mydata.new)
which.max(chem$y)
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", level = 0.90)
summary(best_BIC)
n <- dim(mydata.new)[1]
step(lm.additive,.~.^2,direction="both",k=log(n))
# for AIC (k = 2)
step(lm.additive,.~.^2,direction="both",k=2)
predict(best_BIC, newdata=mydata.new[17,],
interval="confidence", level = 0.90)
# Question 2
animals <- read.csv(file.choose(),header=TRUE)
# 2a
animals$Body_Weight_kg <- animals$Body_Weight / 1000
plot(animals$Body_Weight_kg, animals$Brain_Weight)
plot(animals$Body_Weight_kg, animals$Brain_Weight,
xlim=c(0, 500), ylim=c(0, 1500))
# 2b
model1 <- lm(Brain_Weight~Body_Weight_kg, data=animals)
plot(animals$Body_Weight_kg, animals$Brain_Weight)
abline(model1)
summary(model1)
# 2c
scatter.smooth(residuals(model1)~predict(model1))
# seems to be at least one outlying observation.
# the residuals vs. fitted values plot indicates that the
# constant variance assumption is violated.
# try to zoom in
scatter.smooth(residuals(model1)~predict(model1),
xlim=c(400, 500), ylim=c(-500, 1500))
# 2d
which.max(animals$Body_Weight_kg > 10000)
animals[19,] # blue whale
max_ind <- which.max(animals$Body_Weight_kg > 10000)
animals[max_ind,] # blue whale
predict(model1, newdata=animals[19,])
animals[max_ind,] # blue whale
# 2e
boxcox(model1)
summary(model2)
# use lambda = 0, --> apply a log transformation
model2 <- lm(log(Brain_Weight)~Body_Weight_kg, data=animals)
summary(model2)
3.406e+00
1.047e-04
# 2f
model3 <- lm(log(Brain_Weight)~log(Body_Weight_kg), data=animals)
summary(model3)
# R-squared decreased (worse model fit)
# Multiple R-squared:  0.133,	Adjusted R-squared:  0.1021
plot(animals$Body_Weight_kg, log(animals$Brain_Weight))
abline(model2)
model3 <- lm(log(Brain_Weight)~log(Body_Weight_kg), data=animals)
summary(model3)
plot(log(animals$Body_Weight_kg), log(animals$Brain_Weight))
abline(model3)
# residual plot is better
plot(model3)
> plot(model3)
plot(model3)
# residual plot is much better!
scatter.smooth(residuals(model3)~predict(model3))
plot(log(animals$Body_Weight_kg),
log(animals$Brain_Weight),
pch=(animals$Species_Code + 20))
abline(model3)
# use Species_Code as a predictor
model4 <- lm(log(Brain_Weight)~log(Body_Weight_kg) + as.factor(Species_Code),
data=animals)
summary(model4)
setwd("/Users/annalieb/Documents/Thesis/")
labels <- read.csv("coverage_by_unique_headline.csv")
pwd
getwd()
setwd("/Users/annalieb/Documents/Thesis/Senior-Thesis")
labels <- read.csv("coverage_by_unique_headline.csv")
stances <- labels$stance
actor <- labels$actor
labels <- labels[, 6:7]
table(labels)
table(labels)
table(stances)
table(actor)
table(labels) / 11704
sum(table(labels) / 11704)
(table(labels) / 11704) * 100
table(actor) / 11704
table(stances) / 11704
(table(stances) / 11704) * 100
(table(actor) / 11704) * 100
# fitted values
summary(VAR(cbind(crt_diff, trans_diff), p=9, type="none"))
# time series analysis for related terms: CRT and trans
# BASED ON: https://otexts.com/fpp2/
library(fpp2)
library(seasonal)
library(urca)
library(lmtest)
library(vars)
setwd("/Users/annalieb/Documents/Thesis/Senior-Thesis")
all_data <- read.csv("related_term_daily.csv")
sum(is.na(all_data)) # 0
# make time series objects
crt <- ts(all_data$CRT_relevant, start=c(2020, 8), frequency=365)
trans <- ts(all_data$trans_relevant, start=c(2020, 8), frequency=365)
# plot daily data
autoplot(crt) +
autolayer(trans)
# moving average with an order of 15 (15-MA)
# helps isolate the trend in the data (time series decomposition)
autoplot(ma(crt, 15)) +
autolayer(ma(trans, 15)) +
scale_color_hue(labels=c("trans mentions")) +
ylab("Proportion of total coverage \n (15-day moving avg)")
# see chapter 6.6 - STL decomposition
crt_trend <- trendcycle(stl(crt, t.window=15, s.window="periodic"))
trans_trend <- trendcycle(stl(trans, t.window=15, s.window="periodic"))
# plot the trend-cycle component
autoplot(crt_trend) +
autolayer(trans_trend) +
scale_color_hue(labels=c("trans"))
# test for log transformation (add one to eliminate zeros)
BoxCox.lambda(crt_trend + 1)
BoxCox.lambda(trans_trend + 1)
# Box Cox suggests lambda = -1 (inverse transformation)
crt_transformed <- (crt_trend + 1)^-1
trans_transformed <- (trans_trend + 1)^-1
autoplot(crt_transformed) +
autolayer(trans_transformed) +
scale_color_hue(labels=c("trans mentions")) +
ylab("\n 1 / (Proportion of total coverage + 1)")
# check for stationarity
summary(ur.kpss(crt_transformed))
summary(ur.kpss(trans_transformed))
# The test statistic (1.2788) is bigger than the 1% critical value (0.739),
# indicating that the null hypothesis is rejected and the data are not stationary.
# We can difference the data, and apply the test again.
# difference (for stationarity)
crt_diff <- diff(crt_transformed)
trans_diff <- diff(trans_transformed)
summary(ur.kpss(crt_diff))
summary(ur.kpss(trans_diff))
# now the test stat is much smaller than the critical value, yay!
# confirm result with ndiffs(crt_trend)
autoplot(crt_diff) +
autolayer(trans_diff) +
scale_color_hue(labels=c("trans mentions")) +
ylab("Differenced \n (1 / (Proportion of total coverage + 1))")
# select optimal number of lags (ie. lag order)
# VARselect() gives four different criteria: AIC, HQ, SC and FPE
# for VAR models, we prefer to use the BIC (same as SC)
# see chapter 11.2 -VAR
VARselect(crt_diff)$selection
VARselect(trans_diff)$selection
# granger causality test
# see https://search.r-project.org/CRAN/refmans/lmtest/html/grangertest.html
grangertest(crt_diff ~ trans_diff, order=9)
grangertest(trans_diff ~ crt_diff, order=9)
# how to interpret:
# https://stats.stackexchange.com/questions/183661/how-to-understand-mutual-granger-causality
# fitted values
summary(VAR(cbind(crt_diff, trans_diff), p=9, type="none"))
# fitted values
summary(VAR(cbind(crt_diff, trans_diff), p=9))
